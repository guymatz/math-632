\documentclass{report}

\input{.tex/preamble}
\input{.tex/macros}
\input{.tex/letterfonts}

\title{
  \Huge{Math 431 - Introduction to Probability Theory}
  \\
  Notes
}
\author{\huge{Guy Matz}}
\date{}

\begin{document}
%\maketitle

% \setcounter{chapter}{1}
\chapter*{20230620 - Renewal Process}

\begin{itemize}
  \item Renewal Process - Video I @ min 59
    \begin{itemize}
      \item $N_t$: Number of arrivals before time t.  It is indexed by
        time, so it is a continuous-time stochastic process
      \item $S_n$: Arrival time of $n^{th}$ "particle"
      \item $X_n$: Inter-arrival time.  Time between $n$ and $n-1$ (iid)
    \end{itemize}
  \item Definitions
    \dfn{ Arrival Time }{
      Let $\{X_i: i = 1,2...\}$ be a sequence o iid strictly positive
      RVs.  Let $S_0 = 0$,
        \[ S_n = X_1 + \dots X_n, n \geq 1 \]
      Then the process
      \[ N_t \text{max} \{ n \geq 0; S_n \leq t \} \]
      is called the Renewal Process corresponding to $\{X_i; i \geq 1 \}$
    }

    \dfn{ Renewal Process }{
      Let $\{X_i : i=1,2...\{$ be a sequence of i.i.d. (See video
          around min 12
          \[ \lim_{t \to \infty} \frac{N_t}{t} = \frac{1}{E[X_1]}   \]
    }

    \dfn{ stochastic process }{
      A series of random variables indexed by time
    }

    \dfn{ IID Process }{
      Independent, Identically distributed RVs
    }

    \dfn{ Renewal-Reward Process}{
      Let $\{N_t: t \geq 0 \}  $ be a renewal process with strictly 
      positive inter-arrival times $\{X_k : k \geq 1 \}$.  Let 
      $\{ Y_n : n \geq 1 \}$ be iid random variables, i.e. "rewards".
      Then the process
      \[ R_t = \sum^{N_t}_{n=1} Y_n, t \geq 0 \]
      is called the Renewal-Reward Process corresponding to 
      inter-arrival times $(X_k)_{k=1}^{\infty}$ and rewards
      $(Y_n)_{n=1}^{\infty}$
    }
    \nt{
      Renewal-Reward Process, whenever a renewal happens, we get a reward

      Generalization of a Renewal process, where the reward is always 1
    }

    \thm{ SLLN for RRP }{
    \[ \lim_{t \to \infty} \frac{R_t}{t}  =  \frac{E[Y_1]}{E[X_1]}  \]
    }

    \item Properties of a Renewal Processes
      \begin{enumerate}
        \item $N_0 = 0$
        \item $N_t$ is increasing in time, and right-continuous
        \item $\mathcal{Z}_{\geq 0}$-valued piece-wise function
        \item $\lim_{t \to \infty} N_t = \infty$
      \end{enumerate}

      \thm{SLLN (Strong Law of Large Numbers) for RP (Renewal Process)} {
          \[ \lim_{t \to \infty} \frac{N_t}{t} = \frac{1}{E[X_1]} \]
      }
      \item Examples: See times (min) in video
        \ex{VIDEO 2, min 28: The Lifeftime of a lightbulb
          $\sim Exp(\frac{1}{100})$. When
        a lightbulb burns out, it takes 1 hour for the technician to 
        notice.  It is then immediately replace.  What is the long-term
        rate off consumption of lightbulbs?}
        {
          Let $L_k$ be the lifeftime of the k-th bulb.
          Then the inter-arrival time is $X_k = L_k + 1$.  $N_t$ is the
          number of bulbs replaced before time t.  By SLLN for RP,
          \[ \lim_{t \to \infty} \frac{N_t}{t} = \frac{1}{E[X_1]}  
          = \frac{1}{E[L1 + 1]} = \frac{1}{100 + 1} \]
          So the long-term replacement rate is 1 bulb every 101 days.
        }

        \ex{VIDEO 2, min 38: Same as above, but the technician only checks
          the bulb exactly on the hour}
        {
          Let $L_k$ be the lifeftime of the k-th bulb.
          Then the inter-arrival time is now an integer, since the
          replacement happens on the hour. 
          $X_k = \lceil L_k \rceil$.  $N_t$ is the
          number of bulbs replaced before time t.  By SLLN for RP,
          \[ \lim_{t \to \infty} \frac{N_t}{t} = \frac{1}{E[X_1]}   \]
          \[ P(X=m) = P(m-1 < L \leq m) = P(L_1 > m-1) - P(L > m)
        = e^{-\frac{1}{100}(m-1)} - e^{-\frac{1}{100}m} \]
        \[ = p^{m-1}(1-p) \]
        I.e. $X_1 \sim Geom(1-e^{-\frac{1}{100}})$.
        So
        \[ E[X_1] = \frac{1}{1 - e^{-\frac{1}{100}}}  \]
        So the long-term replacement rate is 1 over that!
        }

        \ex{VIDEO 2, min 49: \underline{Renewal-Reward Process} - 
          Bus arrives at a station with inter-arrival time of 10 mins, on
          avergae.  The number of customers, on average, get off.  What
          is the long-term rate of passengers that get off at the station?
        }
        {
          Intuitive answer.  3 passengers every 10 mins, or 18 / hr.

          $X_k$ is the interarrival time of the buses. $Y_k$ is the 
          number of passengers that get off the bus (reward).  Let $R_t$
          be the number of people who get off the bus before time t.
          \[ R_t = \sum^{N_t}_{k=1} Y_k \]
          The longterm rate of passengers is
          \begin{align*}
            \lim_{t \to \infty} \frac{R_t}{t} &= \lim_{t \to \infty} \frac{1}{t} \sum^{N_t}_{k=1} Y_k \\
                  &= \lim_{t \to \infty} \frac{N_t}{t} \frac{1}{N_t} 
                             \sum^{N_t}_{k=1} Y_k \\
                  &= \frac{E[Y_1]}{E[X_1]} \\
                  &= \frac{3}{10} 
          \end{align*}
        }
        
        \nt{
          \[ \lim_{t \to \infty} \frac{R_t}{t} = \frac{E[Y_1]}{E[X_1]}  \]
          Where $E[Y_1]$ is the Reward mean, and $E[X_1]$ is the 
          cycle length.

          \textbf{So the long-term ratio of rewards is the mean reward
          per cycle, divided by the mean of cycle length}
        }

      \item Questions
        \begin{itemize}
          \item So, $X_n$ is a time for nth particle,  $S_n$ is total time,
            and $N_t$ is \# of arrivals before time t, an integer
          \item What is the difference between $S_{N_t}$ and $t$?
        \end{itemize}
      

  \end{itemize}

\chapter*{20230621 - Renewal-Reward / Markov Chains}
\section*{Renewal Reward}%

  \ex{On-OFF Process} {
    See min 23 of video 3

    \textit{A machine alternates between "ON" and "OFF".  On avergae it
    spends 5 hours at "ON" state, and 3 hours at "OFFF".  What is the
  limiting proportion of time spent in the "ON" state?}

    We say "a renewal happens" when it is switched from "OF" to "ON"

    Let $O_k, F_k$ denote the time spend in ON, OFF states during the k-th
    renewal cycle.
    \[ X_k = O_k + _k \]

    Consider the time ON as a reward

    \[ \sum^{N_t}_{k=1} O_k \leq R_t \leq \sum^{N_t+1}_{k=1} O_k \]
    ??? Because we're not counting OFF for $R_t$ ???
    \[ \frac{1}{t} \sum^{N_t}_{k=1} O_k \leq frac{R_t}{t} \leq \frac{1}{t} \sum^{N_t+1}_{k=1} O_k \]

   \[ \frac{N_t}{t} \frac{1}{N_t} \sum^{N_t}_{k=1} O_k \leq frac{R_t}{t} \leq \frac{N_t+1}{t} \frac{1}{N_t+1} \sum^{N_t+1}_{k=1} O_k \]
    \[ \frac{1}{E[X_1]} E[O_1]  \]

    See video at min 35 for more

  }

  Take-away message:  The SLLN  for RRP still works even though rewards
  do not come as "packages"

  \section*{Markov Chains : Video 0621 @ min 43}%
  \dfn{ Markov Chain }{
    A discrete-time stochastic process $\{X_t: k \in \mathbb{Z}_{\geq 0}\}$
      with countable state space $S$ is said to be a Markov Chain if
      \begin{align*}
         &P(X_{n+1}=a_{n+1}|X_n=a_n, x_{n-1}=a_{n-1},\dots,X_0=a_0) \\
       = &P(X_{n+1}=a_{n+1}|X_n=a_n)
      \end{align*}

      for all $a_0, a_1, \dots, a_{n+1} \in S, n \geq 0$

      If $P(X_{n+1}=a_n+1|X_n=a_n)$ does not depend in n, then the Markov Chain is said to be "time-homogeneous" (because IID) , and
        \[ P(a,b) := P(X_{n+1}=b|X_n =a ) \]
        is the \underline{transition probability} of jumping rfom a to b.
  }
  \ex{HOW TO CHECK THE MARKOV Property - video 0621, min 51} {
    Every IID process $\{X_k: k \geq 0\}$ is a time-homogeneous MC.
    \[ P(X_{n+1} = a_{n+1} | X_n=a_n, \dots , x_0 = a_0) = 
    P(X_{n+1}=a_{n+1}) \]

    and,
    \[ P(X_{n+1} = a_{n+1} | X_n=a_n) = P(X_{n+1}=a_{n+1}) \]

    So to check MC, verify that 
    \[ P(X_{n+1} = a_{n+1} | X_n=a_n, \dots , x_0 = a_0) = 
     P(X_{n+1} = a_{n+1} | X_n=a_n) \]
   Hence the Markov Property holds and $(X_k)_{k \geq 0}$ is a Markov Chain

   Moreover, because $X$ are iid,
   \[ P(X_{n+1} = b | X_n = a) = P(X_{n+1}=b) =  P(X_1=b) \]
   does not depend on $n$, so the Markov Chain is time-homogeneous, with
   \[ p(a,b) = P(X_1 = b) \]
  }
  
%  \ex{SRW: Video 0621 - min 58} {
%    The Simple Random Walk (SRW) is also a MC.
%
%    Let $(Y_k: k \geq 1)$ be iid RVs with $P(Y_1 = 1) = p$ and
%    $P(Y_1 = -1) = (1-p)$
%
%    Let $S_0 = 0, S_n = \sum^{\infty}_{k=1} , n \geq 1$
%
%    To check the Markov Property, we need to verify the conditioning on all
%    states equals the conditioning on the current state, I.e
%    \[ P(S_{n+1}=a_{n+1} | S_n=a_n,  \dots, S_0 = 0)
%      =  \frac{P(S_{n+1}=a_{n+1} , S_n=a_n,  \dots, S_0 = 0)}
%      {P(S_n=a_n,  \dots, S_0 = 0)}
%      \[  = \frac{P(Y_1=a_1,Y_2=a_2-a_1, \dots Y_{n+1}=a_{n+1}-a_n}
%        {P(Y_1=a_1,Y_2=a_2-a_1, \dots Y_{n}=a_{n}-a_{n-1}}
%      \]
%  }
  \ex{For us to show: @ min 1:09} {
    Let $\{Y_k, k\geq 0\}$ be iid RVs.  Let $X_k=(Y_k-1, Y_k), k \geq 1$.  
    Then $\{X_n, k \leq 1\}$ is a MC.  Show this!
  }

\chapter*{20230622 - Markov Chains}
  \begin{itemize}
    \item Conditioning on the entire history is the same as conditioning
      on the current state
    \dfn{ Markov Chain }{
      Discrete-time Stochastic Process
    }
    \dfn{ Time-Homogeneous }{
      Transition probability does not depend on time.  I.e, if
      \[ P(X_{n+1} = b | n_n=a) = p(a,b) \]
      does not depend on time
    }
    \item Examples
      \item SRW
      \item IID process
    \dfn{ Transition Probability }{
      P of transition from a to b, i.e. $p(a,b)$
    }
    \dfn{ Transition Probability Function }{
      A function $p : S \times S \rightarrow [0,1]$ is called a transition
      probability function if
        \[ \sum^{}_{b \in S} p(a,b) = 1 \]
      for all $a \in S$
    }
    \nt{
      For a time-homogensous MC, $P(X_{n+1}=b| X_n=a) = p(a,b)$
        deffines a transition prob function
    }
    \ex{a Video 20230622} {
      \begin{itemize}
        \item rolling dice @ min 17?

        \item (Biased) SRW @ min 18?
          \begin{align}
            P(a,b) = P(S_{n+1}=b|S_n=a) &= 0 \text{ if } |b-a| > 1 \\
                                       &= p  \text{ if } b=a + 1 \\
                                       &= 1-p  \text{ if } b=a - 
          \end{align}
        \item IID (min 24)

          Let $Y_0, Y_1, \dots$ be iid with prob
          \[ P(Y_0 = 1) = P(Y_0 = -1) = \frac{1}{2} \]
          Then $X_n = (Y_n, Y_{n-1}) , n \geq 1$ is a MC with state space
          \[ S =  \{ (-1, -1), (-1, 1), (1, -1), (1, 1) \} = \{-1,1\}^2\]
          Its' Transition Probabiity Function
          \begin{align}
            P( (a_1, a_2), (b_1, b_2) &= P(X_{n+1} = (b_1, b_2) | X_n = (a_1, a_2)) \\
                                  &= P(Y_{n+1} = b_1, Y_n=b_2| Y_n = a_1, Y_{n-1}=a_2)
                                  &= \frac{P(Y_{n+1} = b_1, Y_n=b_2, Y_n = a_1, Y_{n-1}=a_2)}{P(Y_n=a_1, Y_{n-1} = a_2)}
                                  &= 0 \text{ if } a_1 \neq b_2
                                  &= \frac{1}{2}  \text{ if } a_1 = b_2
          \end{align}
          For all $(a_1, a_2), (b_1, b_2) \in S$
        \item Gambler's Ruin at min 35

          Win with prob p (lose with prob 1-p).  The games stops when
          fortune is 0 or \$5.  $X_0 = x$.  Transition probability function
          \begin{align}
            p(a,b) &= p \text{ if } b = a+1, 1 \leq a \leq 4 \\
                   &= 1-p \text{ if } b = a-1, 1 \leq a \leq 4 \\
                   &= 1 \text{ if } a=b \\
                   &= 0 \text{ otherwise }
          \end{align}

          \dfn{ Absorbing State}{
            If a state $a \in S$ such that $p(a,a) = 1$ , then a
            is called an absorbing state
          }
          \dfn{ Initial Distribution }{
            Let $\mu$ be a prob dist on $S$.  We say that the MC
            $(X_n), n \geq 0$ has initial dist $\mu$ is $P(X_0=x)=\mu(x)
            , x \in S$
          }
          \thm{ title } {
            For any prod dist $\mu$ on $S$ and trans prob func
            $p(\cdot,\cdot)$ there exists a time-homo MC with state space
            $S$ , init dist $\mu$ and trans prob $p(\cdot,\cdot)$
          }
          \ex{@ 1:08} {
            example of Gambler's ruin by MC
          }
      \end{itemize}
      \nt{
        If a MC has initial distribution $\mu$, then it's distribution
        is denoted by $P_{\mu}$
      }
    }
  \end{itemize}

\chapter*{20230626 - Markov Chains}
  \begin{itemize}
    \item  Success Run Chain: success move you forward, failure moves
      back to beginning
    \item $P_a = P_{\delta_a}$: The dist of a MC with initial dist
      $\mu = \delta_a$, i.e. $X_0 = a$ with prob 1
    \item The Expectation with respect to $P_{\mu}$ and $P_a$ are
      denoted by $E_{\mu}$ and $E_a$, respectively
    \item Let $X_0$ be a RV .  Let $(Y_k)_{k \in \mathbb{N}}$ be a seq
      of indep. RVs.  Let $g$ be a certain function.

      Then the RVs $(X_n)_{n \in \mathbb{Z} \geq 0}$ defined by 
      \[ X_{n+1} = g(X_n, Y_{n+1}) \]
      is a Markov Chain

  \end{itemize}

  \section*{Examples}%
    \begin{itemize}
    \item min 22 - Transition Probability Function
    \item min 45 - Success-Run chain with $g$ function above
    \item min 110 - Multi-step MC
    \end{itemize}
  
\chapter{Definitions}%
  \dfn{Stopping Time, p. 39}{
    The RV, $T$  with values $\mathbb{Z}_{\geq0} \cup \{ \infty \}$ is a 
    \underline{Stopping Time} (for the process $X$) if, for each
    nonnegative imteger $n$, there is a subset $C_n \subset S^{n+1}$
    such that
    \[ \{ T = n \} = \{(X_0, \dots , X_n) \in C \} \]
    I.e., for each $n$, the values $(X_0, \dots , X_n)$ determine whether
    $T=n$ happens or not.

    I.E! The first recurrence of some particular thing
  }
\chapter*{20230627 - Markov Chains}
   \begin{itemize}
     \item $E_\mu[f(x)] = \mu P^n f(x) $
     \item Markov into the infinite future
         \[ P_\mu\left( (x_{n+1}, \dots) \in A | X_n=x,(X_k, \dots, X_n) \in B \right) = P_x 
         \left( ( X_{n+1}, \dots) \in A) \right) \]
   \end{itemize}
   \section*{String Markov Property}%
   \dfn{ Stopping Time }{
     A random variable $T \in [0, \infty]$ is said to be a \underline{Stooping Time} if for any $n \geq 0$
     there exists  a set $C_n$ such that
     \[ \{T = n \} = \{ (X_0, \dots , X_n) \in C_n \} \]
     When $C_n$ denotes the set of sequences $(x_1, \dots x_n)$ where $x_1 \neq x, \dots, x_{n-1} \neq x$
     and $x_n = x$
     \ex{$\{T_x = n\}$} {
      \[\{T_x = n\} = \{ X_1 \neq x, X_2 \neq x, \dots, X_n = x\} \]
       So, $X_n = x$
     }
   }
   \nt{
     To verify a \underline{Stopping Time},  justify that $\{T=n\}$ only
     involves the history of the MC up to time n.
   }
   
   \dfn{ Hitting Time }{
     For any set $A \subset S$, we denote the \underline{hitting time} by
     \[ \tau_A = inf \{n \geq 0: X_n \in A \} \]
     Then $\tau$ is also a \underline{Stopping Time} because
     \[ \{ \tau_A = n \} = \{ X_0 \notin A, X_{n-1} \notin A, X_n \in A \} = \{ (X_0, \dots, X_n) \in C_n \} \]
   }
   \dfn{ Strong Markov Peroprty }{
     For a time-homogeneous MC and and set A,B and any $\mu$, and any stopping time T
     \[ P_\mu (  (X_{T+1}, \dots ) \in A | X_T=x, T > \infty , (X_0, \dots, X_T) \in B = P_x( (X_1, X_2 \dots ) \in A) \]
     The probability that the future behaves like A, is the same as a MC starting fresh
   }
   
\chapter*{20230628 - Markov Chains}
  \nt{
    Notation:
    \begin{itemize}
      \item Stopping time, $T $is the $n$ where $X_n = 3$
        \[ T = inf \{n \geq0: X_n = 3\} \]
      \item The probability that at 1 past the Stopping Time $T$, $X_n = 2$, given that the
        value of $X_n$ at the Stopping Time is 3, and the Stopping Time is finite
        \[ P(X_{T+1}=2 | X_T=3, T < \infty) \]
      \item $\rho_{xy}: $ Probability that starting from x, we will visit y in finite time
        \[ \rho_{xy} = P_x(X_n = y \text{ for some  } n \geq 1) \]
      \item Time atfer $k^{th}$ return to x that you return to x, i.e. the k-th return time to x
        \[ T^{k+1}_x = inf \{n > T_x^k : X_n = x \}  \]
      \item k-th return to y
        \[ T^k_y \]
      \item Probability of the k-th return to y after starting from x ?
        \[ P_x(T^k_y) \]
      \item $N_x$ : Total number of visits to x (after time 1)
      \item $E_x[N_y]$: Starting from x, how many times will we visit y
          \[ E_x[N_y] = \sum^{\infty}_{n=1} P(X_n=y) = \sum^{\infty}_{n=1} T_y^k < \infty  \]
          \[ E_x[N_y] = \sum^{\infty}_{k=1} \rho_{xy} \cdot \rho^{k-1}_{yy}   \]
            See min 57 off 20230628 video
      \item $R_x$ is called the "Communicating Class of x"
      \item $T^k_x$: Time to return to x, k times
        \[ P_x(\{T^k_x < \infty \text{ for all } k \geq 1\}) = 0 \]
        Means that, starting at x, the probabity is 0 that the time to return to x k-times is finite

    \end{itemize}
  }
  \section*{Transience nad Recurrence}%
  \dfn{ Transience }{
    If $\rho_{xx} < 1$
    \[ P_x(T_x^k = \infty \text{ for some k } = 1 \]
    \[ P_x(T_x^k < \infty \text{ for all k } = 0 \]
    $N_x < \infty$
    To prove that a state is transient, show that it has a P > 0 of escaping frfom the state, and there
    is a recurrent state with p=1
  }
  \dfn{ Recurrence }{
    If $\rho_{xx} = 1$
    \[ P_x(T^k_x < \infty, \text{ for all  } k \geq 1) = 1 \]
    \[ P_x(T^k_x = \infty, \text{ for all  } k \geq 1) = 0 \]
      \[N_x = \infty \]
      Recurrence is contagious, i.e. any state that communicates with a recurrent state is recurrent
  }
  \[ 
  E_x[N_y] =
  \begin{cases}
    0 &\text{ if } \rho_{xy} = 0 \\
    \infty &\text{ if } \rho_{xy} > 0 \& y \text{ is recurrent } \\
    \frac{p_{xy}}{1 - \rho_{xy}}  &\text{ if } \rho_{xy} > 0 \& y \text{ is transient }
  \end{cases}
\]
  \thm{ $x$ is recurrent iff } {
    $x$ is recurrent $\Leftrightarrow E_x[N_x] = \infty \Leftrightarrow N_x = \infty
    \Leftrightarrow \\sum^{\infty}_{n=1} p^n(x,y) = \infty$
  }
  \myproof {
    The return time to x is > 0, to $T_0 \sim $ Geom(1-p) implies
    $p(0,0) = p_0(T_0 < \infty) = 1$
  }

    \ex{SSRW - Symmwetric Simple Random Walk} {
      SSRW is recurrent, i.e. all states are recurrent..  We only need to show that "0" is recurrent. I.e
      \begin{align}
        \infty = E_0[N_0] &= \sum^{\infty}_{n=1} p^n(0,0) \\
                          &=  \sum^{\infty}_{k=1} p^{2k}(0,0) \\
                          &=  \sum^{\infty}_{k=1} \binom{2k}{k} \frac{1}{2} ^{2k} \\
                          &=  \sum^{\infty}_{k=1} \frac{2k!}{k!k!} \frac{1}{2} ^{2k} \\ \\
                          &=  \text{ Stirling }
      \end{align}
    }
    \section*{ Classification of States}
    \nt{
      \begin{itemize}
        \item $x \rightarrow y$: if $p^n(x,y) > 0$ for some $n \geq 0$ and we say y is accessible from x
        \item $x \leftrightarrow y$: if $x \rightarrow y$ and $x \leftarrow y$, we say x and y communicate
      \end{itemize}
    }
    \thm{ title } {
      \begin{itemize}
        \item if $x \neq y$, $x \rightarrow y$, and $\rho_{yx} < 1$, then x is transient
        \item if $x \neq y$, $x$ is recurrent and $x rightarrow y$, then $\rho_{yx} = 1$, and y is recrrent
      \end{itemize}
    }
\chapter*{20230629 - Markov Chains}
    \dfn{ Irreducible }{
      $A \subset S$ is irredicible if $x \leftrightarrow y$ for all $xx,y \in A$

      We say that $B \subset S$ is closed if for any $x \in B, y \notin B$, we have $x \not\rightarrow y$ 

      $R_x$ is called the "Communicating Class of x"
    }
    `
  \thm{ title } {
    Every finite closed subsets of a state space contains at least one recurrent state

    So every irredicible finite-stae MC is recurrent
  }


\chapter*{20230703 - Markov Chains}
  \dfn{ Canonical Decomposition }{
    Union of transient state  with the union of all recurrent states

    See min 10 of 20230703
  }

  Every finite-time Markov chain has an  - eventually - absorbing state

  \[ m(x) = E_x[\tau] = (I-Q)^{-1} 1_{x} \]
    Where 1 is a column matrix of "1"s.  This is the expected time
    spent in transient states

    $u(x,y)$ is the probability of going from a specific transient
    state to a specific recurrent state. Probabilityof being absorbed

    \section{Invariant Measure}%
    Describes the long-term behaviour , long-term frequency to a state
    \dfn{ Invariant zmeasure }{
     Let $\mathbf{P}=\{p(x, y)\}_{x, y \in \mathcal{S}}$ be a transition probability and $\mu$ a measure on $\mathcal{S}$. Then $\mu$ is an invariant measure for $\mathbf{P}$ if $\mu=\mu \mathbf{P}$ and $\mu$ is neither identically zero nor identically infinite.
As before, the equation $\mu=\mu \mathbf{P}$ means that
$$
\mu(y)=\sum_{x \in \mathcal{S}} \mu(x) p(x, y) \quad \text { for all } y \in \mathcal{S}
$$ 
    }
    \thm{ Multiplicative constant of an invariant distribution } {
     Assume that $\mathbf{P}$ is irreducible and recurrent. Then there is an invariant measure $\mu$ such that $0<\mu(x)<\infty$ for all $x \in S$. The invariant measure is unique up to a constant multiple: that is, if $\nu$ is any other invariant measure, then there is a constant $c \in(0, \infty)$ such that $\nu(x)=c \mu(x)$ for all $x \in S$. 
    }

\chapter{20230706 - Markov Chains}%
   \dfn{Invariant Measure }{
     A measure $\mu$ on $S$ is called a
     \underline{Invariant (or Stationary)}
     measure of the MC is it satifies
     \[ \mu(x) = \sum^{}_{y} \mu(u)p(x,y) \]
   }
   \dfn{Invariant /Stationary Distribution }{
    If an invariant Measure is a probability measure, then is it an invariant / staionary distribution
  }
   
\chapter{20230711 - Markov Chains}%
  \section{Invariant Distributions}%
    \dfn{ Stationary Process}{
      Let $\left\{X_k\right\}_{k \geq 0}$ be a stochastic process with
      countable state space $\mathcal{S}$, defined on a probability space
      $(\Omega, \mathcal{F}, P)$. Then $\left\{X_k\right\}_{k \geq 0}$ is a stationary process if the following is true for all integers $m, n \geq 0$ and all states $x_0, \ldots, x_n \in \mathcal{S}$ :
        \[
        \begin{aligned}
        & P\left(X_0=x_0, X_1=x_1, \ldots, X_n=x_n\right) \\
        & \quad=P\left(X_m=x_0, X_{m+1}=x_1, \ldots, X_{m+n}=x_n\right)
        \end{aligned}
        \]
      I.e, while the state Xn keeps evolving randomly, statistically the process looks the same for all time.

      If by shiftfing the process by time $n$ the distribution of 
      the process does not change
    }
    \thm{ A MC with an invariant Dist is Stationary } {
      Suppose $\pi$ is an invariant distribution for the transition probability $\mathbf{P}$ and we take $\pi$ as the initial distribution of the Markov chain. Then the Markov chain is a stationary process: for all integers $m, n \geq 0$ and all states $x_0, \ldots, x_n \in \mathcal{S}$ :
$$
\begin{array}{r}
P_\pi\left(X_m=x_0, X_{m+1}=x_1, \ldots, X_{m+n}=x_n\right) \\
\quad=P_\pi\left(X_0=x_0, X_1=x_1, \ldots, X_n=x_n\right) .
\end{array}
$$
    }
    \ex{Stationary Sequence} {
      MC whose initial distribution is the invariant distribution
      is stationary
    }
    \dfn{ Positive Recurrent / Null State }{
      A recurrent state $x$ is positive recurrent if $E_x\left[T_x\right]<\infty$ and null recurrent if $E_x\left[T_x\right]=\infty$
    }
    \thm{ Positive Recurrence is Contagious } {
      Let $x$ and $y$ be two communicating states $(x \longleftrightarrow y)$. Then if one is positive recurrent, so is the other.
    }
    \thm{ Invariant Dist $\Leftrightarrow$ All State Pos Rec } {
       Assume that $\mathbf{P}$ is irreducible. Then $\mathbf{P}$ has an
       invariant distribution $\pi$ if and only if all states are
       positive recurrent. In this case the invariant distribution $\pi$
       is unique and its values satisfy
       $\pi(x)=\frac{1}{E_x\left[T_x\right]}$ for all $x \in S$.
    }
    \thm{ Irriducible Finite MC $\implies$ Pos Recurrent } {
      An irreducible finite Markov chain is positive recurrent.
    }

    \nt{
        If a MC have a positive recurrent state, then
        an invariant distribution $\pi$ exists
        \[ \pi(y) = \frac{E_X[\# \text{ of visits to y before }T_x]}{E_X[T_x]}  \]
    }
    \thm{ Existence of unique invariant dist} {
      If a MC is positive recurrent and irreducible then it has a
      \underline{unique} invariant distribution
    }
    \nt{
      For a positive recurrent and irreducible MC
      \[ E_X[T_x] = \frac{1}{\pi(x)}  \]
      and
      \[ \pi(x) = \frac{1}{E_X[T_x]} = \frac{E_y[\# \text{ of visits to x}]}{E_Y[T_y]}  \]
    }
    \thm{ SLLN for MC } {
      If a MC is pos rec and irred. and $f$ is a function on $S$, then
      \[ \lim_{n \to \infty} \frac{1}{n} \sum^{n}_{k=1} f(X_k) = \sum^{}_{x \in S} \pi(x)f(x)  \]
    }

\chapter{20230712 - Markov Chains}%
  \section{Invariant Distribution}%
    \dfn{ Invariant Distribution }{

      Let $\mathbf{P}=\{p(x, y)\}_{x, y \in \mathcal{S}}$ be a transition probability and $\pi$ a probability measure on $\mathcal{S}$. Then $\pi$ is an invariant distribution for transition matrix $\mathbf{P}$ if $\pi=\pi \mathbf{P}$, or more explicitly,
      \[ \pi(y)=\sum_{x \in \mathcal{S}} \pi(x) p(x, y) \quad \text { for all } y \in \mathcal{S} \]

    }
    \nt{
      Alternative terms for the invariant distribution are invariant
      probability measure and stationary distribution.

      Long-term frequency of visits to states

      Long-term frequency of visits to transient states is 0
    }
    \thm{ $P_\pi(X_n=y)=\pi(y)$ } {
     Suppose $\pi$ is an invariant distribution for the transition probability $\mathbf{P}$ and we take $\pi$ as the initial distribution of the Markov chain. Then at each time $n \geq 0$ the distribution of the state $X_n$ is $\pi$. In other words, $P_\pi\left(X_n=y\right)=\pi(y)$ for all $n \geq 0$ and $y \in \mathcal{S}$. 
    }
    \nt{
      If a MC has a rec state, x, then it has an invariant measure
      \[ \mu(x) = E_0[\# \text{ of visits to x before  }T_0] \]
      if, in addition, the MC is irreducibe, then the invariant measure
      is unique up to a multiplicative constant
    }
   \nt{
     See 20230712 @ min 29 for how to write an invariant probability with
     multiple communicating classes

   }
   \ex{}{
       If $x_0=1$ then
       \[ \lim_{n \to \infty} \frac{1}{n} \sum^{n}_{i=1} f(x_i) = f(1) \]
       If $x_0 \in \{2,3\}$, then
       \[
         \lim_{n \to \infty} \frac{1}{n} \sum^{n}_{i=1} f(x_i) =
          \mu(2) \cdot f(2) + \mu(3) \cdot f(3)
        \]
       If $x_0 = 0$, then let
       \[ T = in \{ n \geq 0 : X_n \in \{1,2\} \]
         Note that letting  (rem: $h(1) = 1$)
          \begin{align*}
            h(x) &= P_x(X_T = 1) \\
            h(0) &= \frac{1}{4} h(1) + \frac{1}{4} h(0) + \frac{1}{2} h(2) \\
                 &= \frac{1}{4} \cdot 1 + \frac{1}{4} h(0) + \frac{1}{2} 0 \\
                 &= \frac{1}{3} 
          \end{align*}
          Hence $p_x(X_T \in R_1) = 1/3$, $p_x(X_T \in R_2) = 2/3$

          Therefore, when $X_0=0$,
          \[
         \lim_{n \to \infty} \frac{1}{n} \sum^{n}_{i=1} f(x_i) =
            \begin{cases}
              f(1) \text{ with prob } 1/3 \\
              \frac{8}{17} f(2) + \frac{9}{17} (3) \text{ with prob } 2/3
            \end{cases}
          \]
        }

\chapter{20230713 - Markov Chains}%
  \section{Invariant Distributions}%

  \dfn{ MC Limit Theorem }{
    If there is a irreducible and Pos Recurrent MC, which is aperiodic, 
    then the limiting distribution in the invariant disttribution
  }
  \nt{
    Let
    \[ J_z^n=\sum_{k=1}^n I_{\left\{X_k=z\right\}} \]

    denote the number of visits to $z$ between times 1 and $n$ (inclusive). The normalized quantity $J_z^n / n$ is the frequency of visits to $z$ up to time $n$. The relationship with the random variable $N_z$ introduced in (2.56) is that
    \[ \lim _{n \rightarrow \infty} J_z^n=N_z \]
  }

  \thm{ Return Times in irredudible/Recurrent MCs } {
   Suppose $\mathbf{P}$ is irreducible and recurrent. Let $\mu$ be an arbitrary initial distribution. Let $z$ be any state. Then the following limits hold with probability one:
$$
\lim _{k \rightarrow \infty} \frac{T_z^k}{k}=E_z\left[T_z\right]
$$
and
$$
\lim _{n \rightarrow \infty} \frac{J_z^n}{n}=\frac{1}{E_z\left[T_z\right]}
$$ 
  }
  \nt{ If an invariant dist exists,  Irreducibility guarantees uniquensss

  aperiodicty allows the dist to converge

  Positive recurrence guarantees existence of invariant dist

  }
  \dfn{ Period }{
    For any rec state, x, let
    \[ I_x = \{ n \geq 1: p^n(x,x) > 0 \]
      i.e. $I_x$ is the set of integers $n$ so that it is possible to go
      from $x$ to $x$ in $n$ steps.  The greatest comment divisor (gcd)
      of $I_x$ is called the period of the state $x \in S$.  We denote it
      by $d(x)$.

      For an irriducible MC, all states have the same period.  This is the
      period of the MC.
  }
  \dfn{ Aperiodic }{
    Period of every state is one
  }
  
  \dfn{ Null Recurrent }{
    \[ E_x[T_x] = \infty \]
  }
  
  \nt{
    For a finite-state MC, every rec state is positive recurrent

    SSRW is NOT Positive Recurrent (it's null recurrent)
  }
  \nt{
    Mean return time - Min 33 of 20230713
    \[ E_x[T_x] = \frac{1}{\pi(c)}  \]
  }
  \dfn{Doubly Stochastic}{
    A transition probability $\mathbf{P}$ is doubly stochastic if 
$$
\sum_{x \in \mathcal{S}} p(x, y)=1 \quad \text { for all states } y \in \mathcal{S}
$$
A little more vividly, in a doubly stochastic matrix both rows and columns sum to one.
  }
  \nt{
    If the state space is finite, then the MC has an invariant distribution
    of the MC is the uniform distribution over the state space

    See video 20230713 @ min 43
  }
  \thm{ Doubly Stochastic + Constant Measure = Invariant } {
     Assume $\mathbf{P}$ is doubly stochastic. Then every constant measure is invariant. If the state space is finite then there is an invariant distribution.
  }
  \nt{
    a Doubly Stochastic MC has no transient states
  }
  \thm{Invariant State with $\pi(x) > 0$ is recurrent}{
     Suppose $\mathbf{P}$ has an invariant distribution $\pi$ and $x$ is a state that satisfies $\pi(x)>0$. Then $x$ is a recurrent state.
  }
  \thm{ Irreducible + Recurrent $\implies$ Invariant } {
   Let $\mathbf{P}$ be the transition matrix of an irreducible and recurrent Markov chain on the state space $\mathcal{S}$. Then for all $x, y \in \mathcal{S}, 0<\lambda_x(y)<\infty$ and $\lambda_x(x)=1$. For all $x \in \mathcal{S}$, the measure $\lambda_x$ is invariant: $\lambda_x=\lambda_x \mathbf{P}$. 
  }

  \dfn{ Reversible MC }{
    20230713 @ min 58

    A measure mu on S is said to be reversible (with respect to the MC)
    if
      \[ \mu(x) \cdot p(x,y) = \mu(y) \cdot p(y,x) \forall x,y \in S\]
      E.g. Simple Random Walk

      In paticular if $\mu$ is a probsability measure that satisfies 
      the equation above, then $\mu$ is a reversible distribution

      This equation is called the detail balance condition
  }
  \thm{ Reversible $\Leftrightarrow$ Invariant Measure } {
    A reversible measure is an invariant measure. A reversible distribution is an invariant distribution
  }

  \dfn{ Measure }{
    function of sets, which gives non-neagtove value to sets.  
    Measure of disjoint union is the sum of the measure
  }

  \dfn{ Probability Measure / Distribution }{
    Measure on the sets so that the measure of the whole set is 1
  }

  \nt{ Every reversible dist/measure is an invariant dist / measure

    E.g. For symmetric SRW, the measure $\mu(x) = 1$ is a reversible
    measure since
      \[ \mu(y)p(y,x) = \mu(x)p(x,y)  \]
  }

\chapter{20230717 - Markov Chains}%
  Detailed Baance Condition
    \[\mu(x)p(x,y) = \mu(y)p(y,x) \forall x,y \in S \]
  To prove reversible is invariant, need to check that summing over flows
  to x equals x, i.e.
    \[ \sum^{}_{y} \mu(y)p(y,x) = \mu(x) \]
    If $\mu$ is reversible, then by detilaed balance condition
    \[ \sum^{}_{y} \mu(y)p(y,x) = \sum^{}_{y} \mu(x)p(x,y) = \mu(x) \sum^{}_{y} p(x,y) = \mu(x)  \]

    SRW has two invariant measures
    \begin{itemize}
      \item $\mu_1(x) = 1, \forall x \in \mathbb{Z} $
      \item $\mu_2(x) = \left( \frac{p}{1-p}  \right)^x$
    \end{itemize}
    Note: $\mu_1$ is not reversible when $p \neq \frac{1}{2} $
    
    But $\mu_2$ is reversible with
    \[ \mu_2(x)p(x,x+1) \neq \mu_2(x+1) p(x+1,x) \]
    since $\mu_2(x)$ is equal on both sides
    
    

    To show that a $\mu$ is not reversible, show that
    \[ \mu_1(x)p(x,x+1) \neq \mu_1(x+1) p(x+1,x) \]
    \thm{  } {
      If the initial distribution of a MC is a reversible distribution,
      $\pi$, then
      \[ P((X_{n+1}, \dots, X_{n+m}) \in A) = P((X_{n+m}, \dots, X_{n+1}) \in A) \forall n,m \in A \]
    }
    \thm{ RW on Conductance . . . } {
      The Random Walk on Conductance model has an Invariant Distribution
      which is reversible
        \[ \pi(x) = \sum^{}_{y} c(x,y) \]
        If the state space is finite, then $\mu$ can be "normalized"
        to be a reversible distribution

        Only need to check that $\mu$ is reversible , i.e.
      \[ \sum_{z}c(x,z) \cdot \frac{c(x,y)}{\sum_{z}c(x,z)} = \sum_{w}c(y,w) \frac{c(y,x)}{\sum_{w} c(y,w) } \]
            implies
        \[ c(x,y) c(y,x) \]
    }
    \nt{
      To convert a reversible measure to a reversible distribution

      (See 20230717, min 43)

      A reversible measure of a MC , e.g.
      \[ \mu(A) = c(A,B) + c(A,C) \]
      can be turned into a reversible distribution, e.g.
    \[ \pi(A) = \frac{\mu(A)}{\sum_{i \in A}c(A,i)}  \]
    }
    \section{Martingales}%
    Generalizes a SSRW, but it's not a MC
    \dfn{ Martingale }{
      For discrete RVs, X, Y  let H be the function defined by
      \[ H(y) = E[X|Y=y] \]
      Then conditional expectation of X given Y is defined as
      \[ E[X|Y] = H(Y) \]
      No longer a dterministic number, but it's a RV
    }
    \ex{min 53} {
      Let X,Y be indep RVs with
      \[ X \sim Poisson(\mu), Y \sim Poisson(\lambda) \]
      \begin{itemize}
        \item Poisson: $P(X=n) = \frac{\mu^n}{n!} e^{\mu}$
        \item $X + Y \sim Poisson(\mu + \lambda)$
      \end{itemize}
      To compute $E[X|X+Y]$,
            \begin{align}
              H(w) &= E[X|X+Y=w] \\
                  &= \sum^{\infty}_{x=0} x P(X=x| X+Y=w)  \\
                  &= \sum^{\infty}_{x=0} \frac{P(X=x, X+Y=w}{P(X+Y=w}  \\
                  &= \sum^{\infty}_{x=0} \frac{P(X=x, Y=w-x}{P(X+Y=w}  \\
                  &= \sum^{\infty}_{x=0} \frac{P(X=x) P(Y=w-x}{P(X+Y=w}  \\
                  \vdots
                  &= \sum^{\infty}_{x=0} \frac{\mu^x \lambda^{w-x}}{(\lambda + \mu)^w} \frac{w!}{x!(w-x)!} x   \\
            \end{align}
    }

\chapter{20230718 - Martingales}%
   \section{Computing Conditional Expectation}%
   To compute $E[X|X+Y]$, first we determine $H(n)$:
            \begin{align*}
              H(n) &= E[X|X+Y=n] \\
                  &= \sum^{\infty}_{x=0} x\frac{\mu^x \lambda^{n-x}}{(\lambda + \mu)^n} \frac{n!}{n!(n-x)!} \\
                  &= \binom{n}{x} \left( \frac{\mu}{\lambda + \mu} \right)^x \left( \frac{\lambda}{\lambda + \mu} \right)^{n-x}
            \end{align*}
            And that's the expectaton of pmf Bin(n, $\frac{\mu}{\lambda + \mu}$) which equals $n \cdot p$, or
            \[ \frac{\mu}{\lambda + \mu} \cdot n \]

    Next we substitute the deterministic input of $H(.)$ by $(X+Y)$ to get conditional expectation
            \[ E[X|X+Y] = H(X+Y) = \frac{\mu}{\lambda + \mu} (X+Y) \]
    See min 13 of 20230718

    Remember $H(n)$ . . .
      \[ H(n) = E[X|X+Y=n] \]
    Properties of conditional expecatation that will be useful
    \begin{enumerate}
      \item If X and Y are independent, then
        \[ E[X|Y] = E[X] \]
      \item $E[X \cdot g(Y)|Y] = g(Y) \cdot E[X|Y]$
      \item $E[E[X|Y]] = E[X]$
      \item For an $a,b \in \mathbb{R}$
        \[ E[aX+bY|Z] = aE[X|Z] + bE[Y|Z] \]
      \item If X and Y are independent
        \[ E[g(X,Y)|Y] = \sum_{x} g(x,Y)P(X=x) \]
    \end{enumerate}
    \ex{} {
      \begin{enumerate}
        \item $X \sim Poi(\mu), Y \sim Poi(\lambda)$ min 33
        \item SSRW $\{S_n\}_{n \geq 0}$ min 36
      \end{enumerate}
    }
    \section{Martingales}%
    A fair game!  Based on the the history your best guess is that it's the same as today
    \dfn{ Martingales }{
      Let $(X_0, X_1, \dots)$ be a sequence of RVs.  A stochastic
      process is asid to be a Martigale if it satisfies
      \begin{enumerate}
        \item $M_n$ is a function of $(X_0, X_1, \dots, X_n)$
        \item $M_n$ has a finite expectation
        \item Martingale Condition: 
          \[ E[M_{n+1}|X_0, \dots, X_n] = M_n \]
          So the best guess based on the past is where you are now
      \end{enumerate}
    }
    \nt{
      We simply say $\left( M_n \right)_{n=0}^{\infty}$ is a
      Martingale if it is a MG with respect to itself
    }
    \ex{} {
      \begin{itemize}
        \item SSRW defined by $\left( S_n \right)_{n=0}^{\infty}$ is a
          martingale with respect to $(Y_1, Y_2, \dots)$

          Check (how to show a sequence is a martingale)
          \begin{itemize}
            \item $S_n = Y_1 + \dots Y_n$ is a function of $(Y_1, \dots, Y_n) $ up to time n
            \item Each term in the sequence has finite expectation.  $-n \leq S_n \ leq n$.  Check!
            \item Check Martingale Property
                \begin{align*}
                  E[S_{n+1} | Y_1, \dots , Y_n]$ &= E[S_n + Y_{n+1}|Y_1, \dots, Y_n]\\
                         &= E[S_n|Y_1, \dots, Y_n] + E[Y_{n+1}|Y_1, \dots, Y_n]\\
                         &= S_n + E[Y_{n+1}] \\
                         &= S_n + 0 \\
                         &= S_n
                \end{align*}
          \end{itemize}
      \end{itemize}
    }
  
    \nt{ If $Y_1, Y_2, \dots$ is a sequence of independent, zero-mean
      RVS, then
      \begin{itemize}
        \item $\left( S_n \right)_{n=0}^{\infty}$ defined by $S_0=0$,
    $S_n = Y_1 + \dots + Y_n$ is a MG with respect to $(Y_1, Y_2, \dots)$
        \item min 1:11 Let H be a sequence of functions $(Y_1, \dots, Y_n)$
          then the sequence $\left( M_n \right)_{n=0}^{\infty}$ defined by 
          \[ M_n = \sum^{n}_{k=1} Y_k \cdot H_k(Y_1, \dots Y_k) \]
          is a Martingale (MG) with respect to $(Y_1, Y_2, \dots)$

      \end{itemize}
  }

\chapter{20230719 - Martingales}%
  \section{Recall}%
    Expected value?  based on past is where you are now
  \[ E[M_{n+1}|X_0, \dots, X_n] = M_n \]
  This only works because $E[X_{n+1}] = 0$?


  Martingales, in general, are not Markov Chains.  If the means of the RVs
  are not 0, we should not expect a MG.  E.g., biased SRW is not a MG

  Martingale Property
  \[ E[M_{n+1} - M_n | X_1, \dots, X_n] = 0 \]
    
  Check that BSRW is NOT a MG - min 26 of 20230719
    \begin{align*}
    E\left[S_{n+1}-S_n \mid Y_1, \cdots, Y_n\right] &= E\left[Y_{n+1} \mid Y_1, \cdots, Y_n\right) \\
          & =E\left[Y_{n+1}\right] \\
          & =1 \cdot p+(-1)(1-p)=2 p-1 \neq 0 \text { since } p \neq \frac{1}{2} .
    \end{align*}

    \section{Polya's Urn}
    See min 35 off 20230719

    There are originally 2 balls in an urn, one yellow, one green.  If
    you pick a yellow ball, you add a yellow.  If you pick a green, then
    you add a green.  It's a biased system.  But it's a Martingale!!  The
    proprtion of green balls in the urn is a MG.

    At start time, $Y_0 = G_0 = 1$.  Let $X_n = (Y_n, G_n)$ be the total
    number of balls.  Then 
      \[ M_n = \frac{G_n}{Y_n + G_n}  \]
    Does this satisfy the MG properties?
    \begin{enumerate}
      \item Is $M_n$ a function all info up to time n?  Yes, since $M_n$
        is a function of $(X_1, \dots, X_n)$
      \item Does each term in the proc have finite expectation?  Yes,
        since $M_n$ is the ration above, so it is between 0 and 1
      \item Martingale Property?  Given the past, up to time n, is
        \[ E[M_{n+1}| ... ] = M_n\] ?
        For any deterministic seqeunce $\{(y_k, g_k)\}$ we compute first
        the P of getting a yellow ball
        \[ P(M_{n+1}) = \frac{g_n}{y_n+g_n+1}   | X_1=(y_1, g_1), \dots X_n=(y_n,g_n)] = \frac{y_n}{y_n+g_n} \]
        Then the P of getting a greeen ball in the n+1 step
        \[ P(M_{n+1}) = \frac{g_n+1}{y_n+g_n+1}   | X_1=(y_1, g_1), \dots X_n=(y_n,g_n)] = \frac{g_n}{y_n+g_n} \]
        Now we can compute conditional expectation 
        \[ E[M_{n+1}|X_1=(y_1, g_1), \dots, X_n=(y_n,g_n)] = \frac{g_n}{y_n+g_n+1}  \cdot \frac{y_n}{y_n+g_n}  + \frac{g_n+1}{y_n+g_n+1}  \cdot  \frac{g_n}{y_n+g_n} \]
        Which becomes
        \[ \frac{g_n}{y_n+g_n}  \]
    Substituting $(y_1,g_1), \dots (y_n,g_n)$by $(Y_1,G_1),
    \dots (Y_n,G_n)$ in the expression above we get
    \[ E[M_{n+1}|X_1, \dots, X_n] = \frac{G_n}{Y_n+G_n} = M_n \]
    \end{enumerate}
    So we have a Martingale!

    \section{Martingale Limit Theorem}%
    Remember, MG is a fair game where E[|Y] is 0, and E[|Y] increments
    are 0
    \thm{ Lemma } {
      If $(M_n)^{\infty}_{n=1}$ is a Martingale, then
      \[ E[M_n] = E[M_1] \]
    }
    \thm{ Martingale Limit Theorem } {
      Let $(M_n)_{n \geq 0}$ be a MG.  If almost surely the sequence
      has a uniform upper bound or lower bound, i,e. there exists a
      constant c such that 
        \[ P(M_n \leq c) = 1 \forall n \]
        or
        \[ P(M_n \geq c) = 1 \forall n \]
        then there exists a RV $M_{\infty}$ such that
        \[ \lim_{n \to \infty} M_n = M_{\infty} \]

        I.e., what is this saying?  That M_n stays within the bound?

        Should this be and?  Not or?

        Polya's urn is an exmaple here.  See min 1:10 of 20230719
    }

\chapter{20230720 - Martingales}%
  \section{Optional Stopping Theorem}%
  Observations.  If $(M_n)^{\infty}_{n=0}$ is a MG
  \begin{itemize}
    \item $E[M_n] = E[M_0]$ for all n
    \item $W_n = \sum^{}_{k=1} (M_k - M_{k-1}) \cdot H_k(M_0, \dost, M_{k-1})$
      This is a MG.  (here $H_k$ is a seq of functions)
      Let's check!  See min 34 of 20230720

      Martingale Propery check.  Verify that
      \[ E[W_{n+1} - W_n | X_0, \dots, X_n ] = 0 \]
  \end{itemize}
  \dfn{ Optional Stopping Time }{
    \begin{itemize}
        \item For any stopping time T of the MG with
          $T \wedge n = \text{ min }\{T,n\}$, i.e.
          \[ \{T \leq n \} = \{(M_0 \dots, M_n) \in C_n\}) \]
          $\left( M_{T \wedge n} \right)^{\infty}_{n=0}$ is still a MG.

        See min 45 in 20230720
      \item $E[M_{T \wedge n}]  = E[M_0]$
    \end{itemize}
  }
  \thm{ Optional Stopping Theorem } {
    Let $(M_n)^{\infty}_{n=0}$ be a MG with respect to 
    $(X_k)^{\infty}_{k=0}$.  Let $T < \infty$ be a stopping time of MG.
    Then we have $E[M_T] = E[M_0]$ is one of the following conditions
    are satisfied
    \begin{enumerate}
      \item T has a deterministic upper bound, i.e.
        \[ \exists c > 0 s.t. P(T \leq c) = 1 \]
        i.e. T is finite
      \item $\exists c > 0$ such that $P(M_{T \wedge n} \leq c) = 1$
        for all n
      \item Increment is finite
        \[ E[T] < \infty \text{ and } P(|M_{T \wedge(n+1)} - M_{T \wedge n} | \leq c) = 1   \]
          for all n
    \end{enumerate}
  }
  \ex{Examples start at min 58} {
    
  }
  
  
%\chapter*{Notation}%
%  \begin{itemize}
%      \item $N_t$: Number of arrivals before time t.  It is indexed by
%        time, so it is a continuous-time stochastic process
%      \item $S_n$: Arrival time of $n^{th}$ "particle"
%      \item $\rho_{xy}: $ Probability that starting from x, we will visit y in finite time
%        \[ \rho_{xy} = P_x(X_n = y \text{ for some  } n \geq 1) \]
%      \item Time atfer $k^{th}$ return to x that you return to x, i.e. the k-th return time to x
%        \[ T^{k+1}_x = inf \{n > T_x^k : X_n = x \}  \]
%      \item k-th return to y
%        \[ T^k_y \]
%      \item Probability of the k-th return to y after starting from x ?
%        \[ P_x(T^k_y) \]
%      \item $N_x$ : Total number of visits to x (after time 1)
%      \item $E_x[N_y]$: Starting from x, how many times will we visit y
%          \[ E_x[N_y] = \sum^{\infty}_{n=1} P(X_n=y) = \sum^{\infty}_{n=1} T_y^k < \infty  \]
%          \[ E_x[N_y] = \sum^{\infty}_{k=1} \rho_{xy} \cdot \rho^{k-1}_{yy}   \]
%            See min 57 off 20230628 video
%      \item $R_x$ is called the "Communicating Class of x"
%      \item $T^k_x$: Time to return to x, k times
%        \[ P_x(\{T^k_x < \infty \text{ for all } k \geq 1\}) = 0 \]
%        Means that, starting at x, the probabity is 0 that the time to return to x k-times is finite
%      \item $T_z^n$: The \# of visits to $z$ beore time $n$
%  \end{itemize}
%
%\chapter*{Definitions}%
%    \dfn{ Arrival Time }{
%      Let $\{X_i: i = 1,2...\}$ be a sequence o iid strictly positive
%      RVs.  Let $S_0 = 0$,
%        \[ S_n = X_1 + \dots X_n, n \geq 1 \]
%      Then the process
%      \[ N_t \text{max} \{ n \geq 0; S_n \leq t \} \]
%      is called the Renewal Process corresponding to $\{X_i; i \geq 1 \}$
%    }
%    \dfn{ Renewal Process }{
%      Let $\{X_i : i=1,2...\{$ be a sequence of i.i.d. (See video
%          around min 12
%          \[ \lim_{t \to \infty} \frac{N_t}{t} = \frac{1}{E[X_1]}   \]
%    }
%
%    \dfn{ stochastic process }{
%      A series of random variables indexed by time
%    }
%
%    \dfn{ IID Process }{
%      Independent, Identically distributed RVs
%    }
%
%    \dfn{ Renewal-Reward Process}{
%      Let $\{N_t: t \geq 0 \}  $ be a renewal process with strictly 
%      positive inter-arrival times $\{X_k : k \geq 1 \}$.  Let 
%      $\{ Y_n : n \geq 1 \}$ be iid random variables, i.e. "rewards".
%      Then the process
%      \[ R_t = \sum^{N_t}_{n=1} Y_n, t \geq 0 \]
%      is called the Renewal-Reward Process corresponding to 
%      inter-arrival times $(X_k)_{k=1}^{\infty}$ and rewards
%      $(Y_n)_{n=1}^{\infty}$
%
%      \thm{ SLLN for RRP }{
%      \[\lim_{t \to \infty} \frac{R_t}{t}  =  \frac{E[Y_1]}{E[X_1]}  \]
%      }
%    }
%  \dfn{ Markov Chain }{
%    A discrete-time stochastic process $\{X_t: k \in \mathbb{Z}_{\geq 0}\}$
%      with countable state space $S$ is said to be a Markov Chain if
%      \begin{align*}
%         &P(X_{n+1}=a_{n+1}|X_n=a_n, x_{n-1}=a_{n-1},\dots,X_0=a_0) \\
%       = &P(X_{n+1}=a_{n+1}|X_n=a_n)
%      \end{align*}
%
%      for all $a_0, a_1, \dots, a_{n+1} \in S, n \geq 0$
%
%      If $P(X_{n+1}=a_n+1|X_n=a_n)$ does not depend in n, then the Markov Chain is said to be "time-homogeneous" (because IID) , and
%        \[ P(a,b) := P(X_{n+1}=b|X_n =a ) \]
%        is the \underline{transition probability} of jumping rfom a to b.
%  }
%    \dfn{ Time-Homogeneous }{
%      Transition probability does not depend on time.  I.e, if
%      \[ P(X_{n+1} = b | n_n=a) = p(a,b) \]
%      does not depend on time
%    }
%    \dfn{ Transition Probability }{
%      P of transition from a to b, i.e. $p(a,b)$
%    }
%    \dfn{ Transition Probability Function }{
%      A function $p : S \times S \rightarrow [0,1]$ is called a transition
%      probability function if
%        \[ \sum^{}_{b \in S} p(a,b) = 1 \]
%      for all $a \in S$
%    }
%     \dfn{ Absorbing State}{
%       If a state $a \in S$ such that $p(a,a) = 1$ , then a
%       is called an absorbing state
%     }
%     \dfn{ Initial Distribution }{
%            Let $\mu$ be a prob dist on $S$.  We say that the MC
%            $(X_n), n \geq 0$ has initial dist $\mu$ is $P(X_0=x)=\mu(x)
%            , x \in S$
%     }
%  \dfn{Stopping Time, p. 39}{
%    The RV, $T$  with values $\mathbb{Z}_{\geq0} \cup \{ \infty \}$ is a 
%    \underline{Stopping Time} (for the process $X$) if, for each
%    nonnegative imteger $n$, there is a subset $C_n \subset S^{n+1}$
%    such that
%    \[ \{ T = n \} = \{(X_0, \dots , X_n) \in C \} \]
%    I.e., for each $n$, the values $(X_0, \dots , X_n)$ determine whether
%    $T=n$ happens or not.
%
%    I.E! The first recurrence of some particular thing
%      E.g Stopping time, $T $is the $n$ where $X_n = 3$ for
%        \[ T = inf \{n \geq0: X_n = 3\} \]
%      \item The probability that at 1 past the Stopping Time $T$, $X_n = 2$, given that the
%        value of $X_n$ at the Stopping Time is 3, and the Stopping Time is finite
%        \[ P(X_{T+1}=2 | X_T=3, T < \infty) \]
%  }
%   \dfn{ Stopping Time }{
%     A random variable $T \in [0, \infty]$ is said to be a \underline{Stooping Time} if for any $n \geq 0$
%     there exists  a set $C_n$ such that
%     \[ \{T = n \} = \{ (X_0, \dots , X_n) \in C_n \} \]
%     When $C_n$ denotes the set of sequences $(x_1, \dots x_n)$ where $x_1 \neq x, \dots, x_{n-1} \neq x$
%     and $x_n = x$
%     \ex{$\{T_x = n\}$} {
%      \[\{T_x = n\} = \{ X_1 \neq x, X_2 \neq x, \dots, X_n = x\} \]
%       So, $X_n = x$
%     }
%   \nt{
%     To verify a \underline{Stopping Time},  justify that $\{T=n\}$ only involves the history
%     of the MC up to time n.
%   }
%   }
%
%   \dfn{ Hitting Time }{
%     For any set $A \subset S$, we denote the \underline{hitting time} by
%     \[ \tau_A = inf \{n \geq 0: X_n \in A \} \]
%     Then $\tau$ is also a \underline{Stopping Time} because
%     \[ \{ \tau_A = n \} = \{ X_0 \notin A, X_{n-1} \notin A, X_n \in A \} = \{ (X_0, \dots, X_n) \in C_n \} \]
%   }
%   \dfn{ Strong Markov Peroprty }{
%     For a time-homogeneous MC and and set A,B and any $\mu$, and any stopping time T
%     \[ P_\mu (  (X_{T+1}, \dots ) \in A | X_T=x, T > \infty , (X_0, \dots, X_T) \in B = P_x( (X_1, X_2 \dots ) \in A) \]
%     The probability that the future behaves like A, is the same as a MC starting fresh
%   }
%  \dfn{ Transience }{
%    If $\rho_{xx} < 1$
%    \[ P_x(T_x^k = \infty \text{ for some k } = 1 \]
%    \[ P_x(T_x^k < \infty \text{ for all k } = 0 \]
%    $N_x < \infty$
%    To prove that a state is transient, show that it has a P > 0 of escaping frfom the state, and there
%    is a recurrent state with p=1
%  }
%  \dfn{ Recurrence }{
%    If $\rho_{xx} = 1$
%    \[ P_x(T^k_x < \infty, \text{ for all  } k \geq 1) = 1 \]
%    \[ P_x(T^k_x = \infty, \text{ for all  } k \geq 1) = 0 \]
%      \[N_x = \infty \]
%      Recurrence is contagious, i.e. any state that communicates with a recurrent state is recurrent
%  }
%    \dfn{ Irreducible }{
%      $A \subset S$ is irredicible if $x \leftrightarrow y$ for all $x,y \in A$
%    }
%    \dfn{ Closed }{
%      We say that $B \subset S$ is closed if for any $x \in B, y \notin B$, we have $x \not\rightarrow y$ 
%
%      $R_x$ is called the "Communicating Class of x"
%    }


\end{document}
